{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Lecture1-NN_sols.ipynb","provenance":[{"file_id":"https://github.com/alpha-davidson/AI4NP/blob/old_materials/activities/Lecture1-NN-Solution.ipynb","timestamp":1610329615914}],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"R_-Q8naoDk36"},"source":["# Neural Networks in TensorFlow's Keras\n","\n","\n","We will use a dense neural network in Keras to solve a simple regression problem. \n","## Learning Task\n","We will construct a dense neural network to predict the invariant mass of a particle from its energy, momentum, (and charge).\n","\n","*Note that this task does not require machine learning. We choose a task with a known mapping to help us create, debug, and tune our first neural network.*\n","\n","## Dataset\n","This dataset is a collection of simulated particle events from [Pythia](http://home.thep.lu.se/~torbjorn/Pythia.html). The dataset is a 2D array where each row represents one event from an $e^{-} + p$ collision. This dataset is comprised _only_ of events where exactly 16 particles are produced from an electron-proton collision. Each particle contains $(p_x,p_y,p_z,E,q)$. Each event is therefore represented by 80 numbers. \n","\n","**Advanced activity:** There are more interesting event-wise learning tasks using this dataset. Consider crafting your own learning task and target for this data.\n","\n","\n","\n","## Computational Notes\n","\n","If this is your first time in a Jupyter-like environment, please read the following carefully:\n","\n"," - You are in an active kernel\n"," - Run each cell with `Shift + Enter`\n"," - You must execute the cells in the order that you want the code to run\n"," - `Runtime`$\\rightarrow$`Change runtime type` allows you to utilize GPUs and TPUs. They are unnecessary here, but will become vital in later exercises.\n"]},{"cell_type":"code","metadata":{"id":"LfL6KNLSDk3-","executionInfo":{"status":"ok","timestamp":1610331530131,"user_tz":300,"elapsed":456,"user":{"displayName":"Michelle Kuchera","photoUrl":"","userId":"09037640902854628765"}}},"source":["# import the packages we will be using\n","import numpy as np\n","import tensorflow as tf\n","import pylab as plt"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"xTx3KTNYGyQu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610330214697,"user_tz":300,"elapsed":1303,"user":{"displayName":"Michelle Kuchera","photoUrl":"","userId":"09037640902854628765"}},"outputId":"8842f811-432e-48f5-80d4-fa57a748a062"},"source":["# import data from github. Note: in colab, go to Files and refresh to see file\n","# here I use Linux commands within the notebook to pull the data file and rename it\n","!wget https://github.com/NuclearTalent/MachineLearningECT/blob/master/doc/pub/Day6/data/homogenous-16-particle-events-energy.npy?raw=true\n","!mv homogenous-16-particle-events-energy.npy?raw=true particle-events.npy"],"execution_count":12,"outputs":[{"output_type":"stream","text":["--2021-01-11 01:56:53--  https://github.com/NuclearTalent/MachineLearningECT/blob/master/doc/pub/Day6/data/homogenous-16-particle-events-energy.npy?raw=true\n","Resolving github.com (github.com)... 140.82.113.4\n","Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://github.com/NuclearTalent/MachineLearningECT/raw/master/doc/pub/Day6/data/homogenous-16-particle-events-energy.npy [following]\n","--2021-01-11 01:56:53--  https://github.com/NuclearTalent/MachineLearningECT/raw/master/doc/pub/Day6/data/homogenous-16-particle-events-energy.npy\n","Reusing existing connection to github.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/NuclearTalent/MachineLearningECT/master/doc/pub/Day6/data/homogenous-16-particle-events-energy.npy [following]\n","--2021-01-11 01:56:53--  https://raw.githubusercontent.com/NuclearTalent/MachineLearningECT/master/doc/pub/Day6/data/homogenous-16-particle-events-energy.npy\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 22986368 (22M) [application/octet-stream]\n","Saving to: ‘homogenous-16-particle-events-energy.npy?raw=true.1’\n","\n","homogenous-16-parti 100%[===================>]  21.92M  47.3MB/s    in 0.5s    \n","\n","2021-01-11 01:56:54 (47.3 MB/s) - ‘homogenous-16-particle-events-energy.npy?raw=true.1’ saved [22986368/22986368]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8-GfZs9bI7Km","executionInfo":{"status":"ok","timestamp":1610330234479,"user_tz":300,"elapsed":545,"user":{"displayName":"Michelle Kuchera","photoUrl":"","userId":"09037640902854628765"}}},"source":["# now we load the data file, which is a numpy array\n","events = np.load(\"particle-events.npy\")"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8O8n_nwTDk3_"},"source":["Recall that each row of this dataset is an entire event. We need each row to represent a training example, which is a single particle.\n","\n","Using `numpy`'s `reshape` method we can make each row represent one particle. "]},{"cell_type":"code","metadata":{"id":"067guDXZDk4A","executionInfo":{"status":"ok","timestamp":1610330237778,"user_tz":300,"elapsed":410,"user":{"displayName":"Michelle Kuchera","photoUrl":"","userId":"09037640902854628765"}}},"source":["# Here we rearrange the data within each of the events to isolate particles\n","\n","#print(\"events[0] =\\n\",events[0])\n","evt_particles = np.reshape(events, (len(events), 16, 5))\n","\n","#print(\"\\nevt_particles[0] =\\n\", evt_particles[0])\n","\n","# Use another call of reshape to combine all events to have the appropriate shape\n","# Complete me:\n","particles = np.reshape(evt_particles,(len(evt_particles)*16,5))\n","\n","#print(\"\\nparticles =\\n\",particles[:10])"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K_YWTBO0Dk4B"},"source":["These are our training data inputs, but we also must provide the targets, which are the invariant masses of each particle. This is a straightforward computation.\n","\n","We choose units where $c = 1$:\n","$$m^2=E^2-||\\textbf{p}||^2$$\n","where $m, E$, and $\\textbf{p}$ are all in GeV.\n","\n","**Create an array of your target values.**\n","Due to insufficient precision, some $m^2$ values for massless particles will come out very slightly negative. These should be treated as zero to avoid `NaN`. I used the `maximum` method from `numpy` to handle this. "]},{"cell_type":"code","metadata":{"id":"ap3EizhuDk4B","executionInfo":{"status":"ok","timestamp":1610332263279,"user_tz":300,"elapsed":438,"user":{"displayName":"Michelle Kuchera","photoUrl":"","userId":"09037640902854628765"}}},"source":["# Complete me:\n","mass_squared = particles[:,3]**2 - (particles[:,0]**2 + particles[:,1]**2 + particles[:,2]**2)\n","\n","mass = np.sqrt(np.maximum(mass_squared,0))"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kprt9pbPDk4D"},"source":["There are several hundred thousand datapoints in this dataset which is overkill for this simple example. Create a test dataset with just 1000 examples."]},{"cell_type":"code","metadata":{"id":"nZkF1erNDk4E","executionInfo":{"status":"ok","timestamp":1610332265072,"user_tz":300,"elapsed":317,"user":{"displayName":"Michelle Kuchera","photoUrl":"","userId":"09037640902854628765"}}},"source":["#Slicing allows you select a subset of an array.\n","#This can be done like this: smallerArray = largerArray[:100]\n","\n","#Complete me:\n","particles_train = particles[:1000]\n","mass_train = mass[:1000]"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5VYyii0yDk4F"},"source":["Next, make a histogram of the target data to make sure that we are seeing masses of real particles. As this data has limited precision, this will not resolve electrons very well, but protons, pions, and massless particles should be clearly visible."]},{"cell_type":"code","metadata":{"id":"xHL4sU3ZDk4G","colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"status":"ok","timestamp":1610332267403,"user_tz":300,"elapsed":629,"user":{"displayName":"Michelle Kuchera","photoUrl":"","userId":"09037640902854628765"}},"outputId":"b456e3fd-23fb-444f-942b-af92f961da35"},"source":["plt.hist(mass_train,bins=100)\n","plt.ylim(0,100)\n","plt.show()"],"execution_count":21,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANO0lEQVR4nO3db4yl9VmH8etbVkRoCxRGggu6mFKR0JjiBGlItHb7ghbDkkgIxOq22XSTWmuVRrvqC4y+gaitNWmqm1K7NhVBbGQj1abZ0jQaIQ6l/wArKwW6yJ9pBfzTaEt6+2KebiabWefMeWbOMDfXJ9nMc855zjl3fpm59tnnnDmbqkKS1MtLNnsASdL6M+6S1JBxl6SGjLskNWTcJakh4y5JDa0a9yQfTvJ0ki8vu+4VST6V5KHh6+nD9UnyR0kOJ/likos3cnhJ0somOXL/CHD5MdftAw5V1fnAoeEywBuB84c/e4EPrs+YkqS1WDXuVfVZ4N+PuXoXcGDYPgBctez6P6sldwOnJTl7vYaVJE1m25T3O6uqnhi2nwTOGra3A19btt+R4bonOEaSvSwd3XPKKaf8+AUXXDDVIF96/Lmj26/efupUjzErW2lWSS98995779eram6l26aN+1FVVUnW/BkGVbUf2A8wPz9fCwsLUz3/jn13Ht1euPGKqR5jVrbSrJJe+JI8erzbpn23zFPfPd0yfH16uP5x4Nxl+50zXCdJmqFp434Q2D1s7wbuWHb9LwzvmrkUeG7Z6RtJ0oyselomyS3A64AzkxwBbgBuBG5Lsgd4FLhm2P0TwJuAw8A3gbduwMySpFWsGvequu44N+1cYd8C3jF2KEnSOP6GqiQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ2NinuSX01yf5IvJ7klyUlJzktyT5LDSW5NcuJ6DStJmszUcU+yHfhlYL6qLgJOAK4FbgLeV1WvBJ4B9qzHoJKkyY09LbMN+L4k24CTgSeA1wO3D7cfAK4a+RySpDWaOu5V9Tjw+8BjLEX9OeBe4Nmqen7Y7QiwfaX7J9mbZCHJwuLi4rRjSJJWMOa0zOnALuA84AeAU4DLJ71/Ve2vqvmqmp+bm5t2DEnSCsaclnkD8NWqWqyqbwMfBy4DThtO0wCcAzw+ckZJ0hqNiftjwKVJTk4SYCfwAHAXcPWwz27gjnEjSpLWasw593tYeuH0c8CXhsfaD7wHuD7JYeAM4OZ1mFOStAbbVt/l+KrqBuCGY65+GLhkzONKksbxN1QlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpoW2bPYAkvdjs2Hfn0e1HbrxiQ55j1JF7ktOS3J7kn5M8mOS1SV6R5FNJHhq+nr5ew0qSJjP2tMz7gb+rqguAHwMeBPYBh6rqfODQcFmSNENTxz3JqcBPAjcDVNW3qupZYBdwYNjtAHDV2CElSWsz5sj9PGAR+NMk9yX5UJJTgLOq6olhnyeBs1a6c5K9SRaSLCwuLo4YQ5J0rDFx3wZcDHywql4D/DfHnIKpqgJqpTtX1f6qmq+q+bm5uRFjSJKONSbuR4AjVXXPcPl2lmL/VJKzAYavT48bUZK0VlPHvaqeBL6W5EeGq3YCDwAHgd3DdbuBO0ZNKElas7Hvc38n8LEkJwIPA29l6S+M25LsAR4Frhn5HJKkNRoV96r6PDC/wk07xzyuJGkcP35Akhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhkbHPckJSe5L8jfD5fOS3JPkcJJbk5w4fkxJ0lqsx5H7u4AHl12+CXhfVb0SeAbYsw7PIUlag1FxT3IOcAXwoeFygNcDtw+7HACuGvMckqS1G3vk/ofArwPfGS6fATxbVc8Pl48A21e6Y5K9SRaSLCwuLo4cQ5K03NRxT/IzwNNVde8096+q/VU1X1Xzc3Nz044hSVrBthH3vQy4MsmbgJOAlwPvB05Lsm04ej8HeHz8mJKktZj6yL2qfqOqzqmqHcC1wKer6ueAu4Crh912A3eMnlKStCYb8T739wDXJznM0jn4mzfgOSRJ/48xp2WOqqrPAJ8Zth8GLlmPx5UkTcffUJWkhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNTR13JOcm+SuJA8kuT/Ju4brX5HkU0keGr6evn7jSpImMebI/Xng3VV1IXAp8I4kFwL7gENVdT5waLgsSZqhqeNeVU9U1eeG7f8EHgS2A7uAA8NuB4Crxg4pSVqbdTnnnmQH8BrgHuCsqnpiuOlJ4Kzj3GdvkoUkC4uLi+sxhiRpMDruSV4K/BXwK1X1H8tvq6oCaqX7VdX+qpqvqvm5ubmxY0iSlhkV9yTfw1LYP1ZVHx+ufirJ2cPtZwNPjxtRkrRWY94tE+Bm4MGqeu+ymw4Cu4ft3cAd048nSZrGthH3vQz4eeBLST4/XPebwI3AbUn2AI8C14wbUZK0VlPHvar+Hshxbt457eNKksbzN1QlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkNjPhVyy9ix786j24/ceMUmTiJJs+GRuyQ1ZNwlqSHjLkkNGXdJasi4S1JDL4p3yyy3/J0z4LtnJPXkkbskNWTcJakh4y5JDRl3SWqo7Quqx75wKkkvJh65S1JDrY7cPVqXpCUeuUtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNtXqf+1bif9otaSN55C5JDW1I3JNcnuQrSQ4n2bcRzyFJOr51j3uSE4APAG8ELgSuS3Lhej+PJOn4NuLI/RLgcFU9XFXfAv4C2LUBzyNJOo6NeEF1O/C1ZZePAD9x7E5J9gJ7h4v/leQrUz7fmcDXp7wvuWnae66fdZhh1Bo04Rq4BrAF12Dkz/8PHe+GTXu3TFXtB/aPfZwkC1U1vw4jbVmugWsArgG4BsttxGmZx4Fzl10+Z7hOkjQjGxH3fwLOT3JekhOBa4GDG/A8kqTjWPfTMlX1fJJfAj4JnAB8uKruX+/nWWb0qZ0GXAPXAFwDcA2OSlVt9gySpHXmb6hKUkPGXZIa2jJxX+0jDZJ8b5Jbh9vvSbJj9lNurAnW4PokDyT5YpJDSY77HtitatKPtkjys0kqSbu3xU2yBkmuGb4X7k/y57OecaNN8LPwg0nuSnLf8PPwps2Yc1NV1Qv+D0svzP4r8MPAicAXgAuP2ecXgT8etq8Fbt3suTdhDX4aOHnYfvuLcQ2G/V4GfBa4G5jf7Lk34fvgfOA+4PTh8vdv9tybsAb7gbcP2xcCj2z23LP+s1WO3Cf5SINdwIFh+3ZgZ5LMcMaNtuoaVNVdVfXN4eLdLP2OQSeTfrTF7wI3Af8zy+FmZJI1eBvwgap6BqCqnp7xjBttkjUo4OXD9qnAv81wvheErRL3lT7SYPvx9qmq54HngDNmMt1sTLIGy+0B/nZDJ5q9VdcgycXAuVV1Jz1N8n3wKuBVSf4hyd1JLp/ZdLMxyRr8NvDmJEeATwDvnM1oLxz+Zx0NJXkzMA/81GbPMktJXgK8F3jLJo+y2baxdGrmdSz96+2zSV5dVc9u6lSzdR3wkar6gySvBT6a5KKq+s5mDzYrW+XIfZKPNDi6T5JtLP1T7BszmW42JvpYhyRvAH4LuLKq/ndGs83KamvwMuAi4DNJHgEuBQ42e1F1ku+DI8DBqvp2VX0V+BeWYt/FJGuwB7gNoKr+ETiJpQ8Ve9HYKnGf5CMNDgK7h+2rgU/X8GpKE6uuQZLXAH/CUti7nWeFVdagqp6rqjOrakdV7WDpdYcrq2phc8bdEJP8LPw1S0ftJDmTpdM0D89yyA02yRo8BuwESPKjLMV9caZTbrItEffhHPp3P9LgQeC2qro/ye8kuXLY7WbgjCSHgeuBVv8D1IRr8HvAS4G/TPL5JK0+02fCNWhtwjX4JPCNJA8AdwG/VlVt/hU74Rq8G3hbki8AtwBvaXawtyo/fkCSGtoSR+6SpLUx7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJauj/APCLDHwhjBVkAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"i-MXhgPQDk4H"},"source":["Now we can build and train the first neural network. We will start with a network with on hidden layer and 5 neurons, and ReLU activation. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"d_MsoqFvDk4H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610332825382,"user_tz":300,"elapsed":266,"user":{"displayName":"Michelle Kuchera","photoUrl":"","userId":"09037640902854628765"}},"outputId":"6eb91415-d7b7-43fe-b5e2-9cc231601d4d"},"source":["# Build our model\n","model = tf.keras.Sequential() #Define the model object\n","model.add(tf.keras.layers.Dense(20, input_shape=(5,), activation=\"relu\")) #Add the hidden layer\n","model.add(tf.keras.layers.Dense(20, input_shape=(5,), activation=\"relu\")) #Add the hidden layer\n","# Add the output layer yourself\n","# No activation function assignment defaults to \"linear\"\n","# Complete me:\n","model.add(tf.keras.layers.Dense(1))\n","\n","model.summary()"],"execution_count":60,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_6 (Dense)              (None, 20)                120       \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 20)                420       \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 1)                 21        \n","=================================================================\n","Total params: 561\n","Trainable params: 561\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xFb0E6GtSK7Z"},"source":["To start, train for 30 epochs with a batch size of 128, an Adam optimizer with a learning rate of 0.1, using mean squared error loss. **This is not ideal.**\n","\n","Define your validation split to provide a set of data the model has not used to assess how the model is generalizing. This allows us to monitor overtraining. Setting `validation_split=0.2` meaning that $20\\%$ of the data will be used for validation.\n","\n","Information on how to implement these features can be found here:\n","<https://www.tensorflow.org/api_docs/python/tf/keras>.\n","Check out Sequential underneath models and Dense under layers."]},{"cell_type":"code","metadata":{"id":"hCp3EXCTDk4I","executionInfo":{"status":"ok","timestamp":1610333129256,"user_tz":300,"elapsed":802,"user":{"displayName":"Michelle Kuchera","photoUrl":"","userId":"09037640902854628765"}}},"source":["optimize = tf.keras.optimizers.Adam(lr=0.1) # Adam optimizer with a learning rate of 0.1\n","loss_func = tf.keras.losses.MeanSquaredError() # choosing MSE\n","model.compile(optimizer=optimize,loss=loss_func) \n"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zs_ReONMDk4I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610332846182,"user_tz":300,"elapsed":5611,"user":{"displayName":"Michelle Kuchera","photoUrl":"","userId":"09037640902854628765"}},"outputId":"8f929233-7f45-47b1-d429-9d72735d9c3c"},"source":["results = model.fit(particles_train, mass_train, epochs=100, batch_size=256, validation_split=0.2)"],"execution_count":62,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","4/4 [==============================] - 0s 46ms/step - loss: 0.3410 - val_loss: 0.3607\n","Epoch 2/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.2527 - val_loss: 0.2669\n","Epoch 3/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.2029 - val_loss: 0.2061\n","Epoch 4/100\n","4/4 [==============================] - 0s 12ms/step - loss: 0.1481 - val_loss: 0.1659\n","Epoch 5/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.1404 - val_loss: 0.1379\n","Epoch 6/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.1175 - val_loss: 0.1177\n","Epoch 7/100\n","4/4 [==============================] - 0s 12ms/step - loss: 0.1115 - val_loss: 0.1031\n","Epoch 8/100\n","4/4 [==============================] - 0s 12ms/step - loss: 0.0970 - val_loss: 0.0930\n","Epoch 9/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0843 - val_loss: 0.0866\n","Epoch 10/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0742 - val_loss: 0.0827\n","Epoch 11/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0773 - val_loss: 0.0792\n","Epoch 12/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0735 - val_loss: 0.0752\n","Epoch 13/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0675 - val_loss: 0.0708\n","Epoch 14/100\n","4/4 [==============================] - 0s 12ms/step - loss: 0.0685 - val_loss: 0.0676\n","Epoch 15/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0622 - val_loss: 0.0650\n","Epoch 16/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0594 - val_loss: 0.0626\n","Epoch 17/100\n","4/4 [==============================] - 0s 15ms/step - loss: 0.0563 - val_loss: 0.0605\n","Epoch 18/100\n","4/4 [==============================] - 0s 12ms/step - loss: 0.0605 - val_loss: 0.0589\n","Epoch 19/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0608 - val_loss: 0.0577\n","Epoch 20/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0565 - val_loss: 0.0565\n","Epoch 21/100\n","4/4 [==============================] - 0s 12ms/step - loss: 0.0570 - val_loss: 0.0551\n","Epoch 22/100\n","4/4 [==============================] - 0s 12ms/step - loss: 0.0532 - val_loss: 0.0536\n","Epoch 23/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0467 - val_loss: 0.0520\n","Epoch 24/100\n","4/4 [==============================] - 0s 53ms/step - loss: 0.0456 - val_loss: 0.0503\n","Epoch 25/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0489 - val_loss: 0.0482\n","Epoch 26/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0464 - val_loss: 0.0465\n","Epoch 27/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0426 - val_loss: 0.0450\n","Epoch 28/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0390 - val_loss: 0.0438\n","Epoch 29/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0385 - val_loss: 0.0425\n","Epoch 30/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0377 - val_loss: 0.0415\n","Epoch 31/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0371 - val_loss: 0.0406\n","Epoch 32/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0337 - val_loss: 0.0397\n","Epoch 33/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0351 - val_loss: 0.0389\n","Epoch 34/100\n","4/4 [==============================] - 0s 15ms/step - loss: 0.0310 - val_loss: 0.0381\n","Epoch 35/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0293 - val_loss: 0.0374\n","Epoch 36/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0291 - val_loss: 0.0365\n","Epoch 37/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0280 - val_loss: 0.0358\n","Epoch 38/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0312 - val_loss: 0.0353\n","Epoch 39/100\n","4/4 [==============================] - 0s 15ms/step - loss: 0.0284 - val_loss: 0.0345\n","Epoch 40/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0290 - val_loss: 0.0340\n","Epoch 41/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0270 - val_loss: 0.0333\n","Epoch 42/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0250 - val_loss: 0.0325\n","Epoch 43/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0271 - val_loss: 0.0316\n","Epoch 44/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0259 - val_loss: 0.0303\n","Epoch 45/100\n","4/4 [==============================] - 0s 12ms/step - loss: 0.0235 - val_loss: 0.0298\n","Epoch 46/100\n","4/4 [==============================] - 0s 15ms/step - loss: 0.0248 - val_loss: 0.0293\n","Epoch 47/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0232 - val_loss: 0.0288\n","Epoch 48/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0244 - val_loss: 0.0284\n","Epoch 49/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0229 - val_loss: 0.0277\n","Epoch 50/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0216 - val_loss: 0.0269\n","Epoch 51/100\n","4/4 [==============================] - 0s 12ms/step - loss: 0.0221 - val_loss: 0.0264\n","Epoch 52/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0213 - val_loss: 0.0259\n","Epoch 53/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0209 - val_loss: 0.0254\n","Epoch 54/100\n","4/4 [==============================] - 0s 12ms/step - loss: 0.0208 - val_loss: 0.0251\n","Epoch 55/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0202 - val_loss: 0.0247\n","Epoch 56/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0205 - val_loss: 0.0244\n","Epoch 57/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0193 - val_loss: 0.0240\n","Epoch 58/100\n","4/4 [==============================] - 0s 15ms/step - loss: 0.0209 - val_loss: 0.0238\n","Epoch 59/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0194 - val_loss: 0.0241\n","Epoch 60/100\n","4/4 [==============================] - 0s 17ms/step - loss: 0.0189 - val_loss: 0.0237\n","Epoch 61/100\n","4/4 [==============================] - 0s 16ms/step - loss: 0.0167 - val_loss: 0.0232\n","Epoch 62/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0193 - val_loss: 0.0230\n","Epoch 63/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0186 - val_loss: 0.0226\n","Epoch 64/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0182 - val_loss: 0.0221\n","Epoch 65/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0170 - val_loss: 0.0221\n","Epoch 66/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0191 - val_loss: 0.0218\n","Epoch 67/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0165 - val_loss: 0.0213\n","Epoch 68/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0172 - val_loss: 0.0210\n","Epoch 69/100\n","4/4 [==============================] - 0s 55ms/step - loss: 0.0165 - val_loss: 0.0209\n","Epoch 70/100\n","4/4 [==============================] - 0s 15ms/step - loss: 0.0174 - val_loss: 0.0208\n","Epoch 71/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0165 - val_loss: 0.0208\n","Epoch 72/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0161 - val_loss: 0.0211\n","Epoch 73/100\n","4/4 [==============================] - 0s 15ms/step - loss: 0.0161 - val_loss: 0.0214\n","Epoch 74/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0148 - val_loss: 0.0208\n","Epoch 75/100\n","4/4 [==============================] - 0s 12ms/step - loss: 0.0156 - val_loss: 0.0203\n","Epoch 76/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0151 - val_loss: 0.0201\n","Epoch 77/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0151 - val_loss: 0.0200\n","Epoch 78/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0158 - val_loss: 0.0200\n","Epoch 79/100\n","4/4 [==============================] - 0s 16ms/step - loss: 0.0156 - val_loss: 0.0197\n","Epoch 80/100\n","4/4 [==============================] - 0s 15ms/step - loss: 0.0151 - val_loss: 0.0194\n","Epoch 81/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0151 - val_loss: 0.0192\n","Epoch 82/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0145 - val_loss: 0.0192\n","Epoch 83/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0136 - val_loss: 0.0192\n","Epoch 84/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0135 - val_loss: 0.0192\n","Epoch 85/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0160 - val_loss: 0.0191\n","Epoch 86/100\n","4/4 [==============================] - 0s 16ms/step - loss: 0.0148 - val_loss: 0.0188\n","Epoch 87/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0147 - val_loss: 0.0187\n","Epoch 88/100\n","4/4 [==============================] - 0s 12ms/step - loss: 0.0135 - val_loss: 0.0188\n","Epoch 89/100\n","4/4 [==============================] - 0s 12ms/step - loss: 0.0122 - val_loss: 0.0188\n","Epoch 90/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0127 - val_loss: 0.0187\n","Epoch 91/100\n","4/4 [==============================] - 0s 12ms/step - loss: 0.0146 - val_loss: 0.0183\n","Epoch 92/100\n","4/4 [==============================] - 0s 12ms/step - loss: 0.0141 - val_loss: 0.0180\n","Epoch 93/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0128 - val_loss: 0.0178\n","Epoch 94/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0142 - val_loss: 0.0178\n","Epoch 95/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0122 - val_loss: 0.0182\n","Epoch 96/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0143 - val_loss: 0.0179\n","Epoch 97/100\n","4/4 [==============================] - 0s 13ms/step - loss: 0.0132 - val_loss: 0.0175\n","Epoch 98/100\n","4/4 [==============================] - 0s 14ms/step - loss: 0.0129 - val_loss: 0.0173\n","Epoch 99/100\n","4/4 [==============================] - 0s 18ms/step - loss: 0.0133 - val_loss: 0.0172\n","Epoch 100/100\n","4/4 [==============================] - 0s 16ms/step - loss: 0.0133 - val_loss: 0.0173\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Mcg4Qj1wZDxE"},"source":["A learning curve is a clearer way to visualize the above training. This helps us determine over and underfitting, which, in turn, helps us tune our model and training."]},{"cell_type":"code","metadata":{"id":"TrIo4jsqDk4J","executionInfo":{"status":"ok","timestamp":1610334203686,"user_tz":300,"elapsed":301,"user":{"displayName":"Michelle Kuchera","photoUrl":"","userId":"09037640902854628765"}}},"source":["# helper function for easy plotting\n","def plot_learning_curve(history):\n","    plt.plot(history[\"loss\"], label=\"training loss\")\n","    plt.plot(history[\"val_loss\"], label=\"validation loss\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.yscale('log')\n","    plt.legend()\n","    plt.show()\n"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"RfhycMMXZahi","executionInfo":{"status":"ok","timestamp":1610334209831,"user_tz":300,"elapsed":615,"user":{"displayName":"Michelle Kuchera","photoUrl":"","userId":"09037640902854628765"}},"outputId":"7fc36189-6056-4dc0-bc13-765e64555a7c"},"source":["# using the new function\n","plot_learning_curve(results.history)"],"execution_count":68,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3/8dc3k33fSCALJBCWkAQS1iAim1YQwX1ppVbrUrlarb1atddq7a2/W2+51otVe92tK4qACwi4gICo7ISwBpJAyAJZyL7PfH9/nCFFTIYsk5xk8nk+HnmYOTNz5nM4PvKe7/l+z/ertNYIIYQQbXEzuwAhhBC9mwSFEEIIhyQohBBCOCRBIYQQwiEJCiGEEA65m11AdwgPD9dxcXFmlyGEEH3Gjh07SrTWA1p7ziWDIi4uju3bt5tdhhBC9BlKqWNtPSeXnoQQQjgkQSGEEMIhCQohhBAOuWQfhRCi5zU1NXHixAnq6+vNLkU44O3tTUxMDB4eHu1+j0sFhVJqPjA/ISHB7FKE6HdOnDhBQEAAcXFxKKXMLke0QmtNaWkpJ06cID4+vt3vc6lLT1rrT7TWdwYFBZldihD9Tn19PWFhYRISvZhSirCwsA63+lwqKIQQ5pKQ6P06c44kKM7y+jc5fJpRYHYZQgjRq0hQnGXp9hMs35lvdhlCiE4oLy/n+eef79R7L7vsMsrLyx2+5rHHHuOLL77o1P7PFRcXR0lJiVP21RMkKM7y7/qfpBYsNbsMIUQnOAqK5uZmh+9dvXo1wcHBDl/zpz/9iYsvvrjT9fVlEhRnGd28jwn1W2iy2swuRQjRQQ8//DBHjx4lNTWVBx98kA0bNjBt2jQWLFjA6NGjAbjyyisZP348SUlJvPjiiy3vPfMNPzc3l8TERO644w6SkpL4yU9+Ql1dHQC33HILy5Yta3n9448/zrhx40hJSeHgwYMAFBcXc8kll5CUlMTtt9/OkCFDzttyePrpp0lOTiY5OZlnnnkGgJqaGubNm8fYsWNJTk5m6dKlLcc4evRoxowZwwMPPODcf0AHXGp4bFc1hI5iRNXnnDhdR3y4n9nlCNFnPfHJPvYXVDp1n6OjAnl8flKbz//lL38hMzOT3bt3A7BhwwZ27txJZmZmy1DQV199ldDQUOrq6pg4cSLXXHMNYWFhP9hPVlYW7777Li+99BLXX389H374IQsXLvzR54WHh7Nz506ef/55Fi9ezMsvv8wTTzzBrFmzeOSRR1izZg2vvPKKw2PasWMHr732Gt9//z1aayZPnsz06dPJzs4mKiqKVatWAVBRUUFpaSkrVqzg4MGDKKXOe6nMmaRFcRb3gaMJV5WcOJFrdilCCCeYNGnSD+4XWLJkCWPHjiU9PZ28vDyysrJ+9J74+HhSU1MBGD9+PLm5ua3u++qrr/7RazZv3syNN94IwJw5cwgJCXFY3+bNm7nqqqvw8/PD39+fq6++mk2bNpGSksLnn3/OQw89xKZNmwgKCiIoKAhvb29uu+02li9fjq+vb0f/OTpNWhRnCYpLha1QdSwDUtv+5iKEcMzRN/+e5Of3rysDGzZs4IsvvuDbb7/F19eXGTNmtHo/gZeXV8vvFoul5dJTW6+zWCzn7QPpqBEjRrBz505Wr17No48+yuzZs3nsscfYunUrX375JcuWLePvf/87X331lVM/ty3SojhLQGwKAPrkfpMrEUJ0VEBAAFVVVW0+X1FRQUhICL6+vhw8eJDvvvvO6TVMnTqV999/H4B169Zx+vRph6+fNm0aK1eupLa2lpqaGlasWMG0adMoKCjA19eXhQsX8uCDD7Jz506qq6upqKjgsssu429/+xt79uxxev1tkRbFWVRAJOUqCN/yw2aXIoTooLCwMKZOnUpycjJz585l3rx5P3h+zpw5/OMf/yAxMZGRI0eSnp7u9Boef/xxfvrTn/Lmm28yZcoUBg4cSEBAQJuvHzduHLfccguTJk0C4PbbbyctLY21a9fy4IMP4ubmhoeHBy+88AJVVVVcccUV1NfXo7Xm6aefdnr9bVFa6x77sJ4yYcIE3dmFiw7/90ya6qpIelwWPhKiIw4cOEBiYqLZZZiqoaEBi8WCu7s73377LYsWLWrpXO9NWjtXSqkdWusJrb1eWhTnqA0ewfCaFdQ3NuHt2f7ZFYUQ4vjx41x//fXYbDY8PT156aWXzC7JKSQozqEiR+NX8B5Hcw8ybESK2eUIIfqQ4cOHs2vXLrPLcDrpzD6Hf+wYACpye66jSAghejMJinNEJhjjp5sK95lciRBC9A4uFRRKqflKqRcrKio6vQ//wBAKiMCr9KATKxNCiL7LpYLCWQsXFXjFE1Z7xElVCSFE3+ZSQeEslYEjGNScD80NZpcihOhG/v7+ABQUFHDttde2+poZM2ZwvuH2zzzzDLW1tS2P2zNteXv88Y9/ZPHixV3eT1dJULTCNiARd6xUFxwwuxQhRA+IiopqmRm2M84NivZMW96XSFC0wifGGBZblu16w9yEcFUPP/wwzz33XMvjM9/Gq6urmT17dsuU4B999NGP3pubm0tycjIAdXV13HjjjSQmJnLVVVf9YK6nRYsWMWHCBJKSknj88ccBY6LBgoICZs6cycyZM4EfLkzU2jTijqYzb8vu3btJT09nzJgxXHXVVS3TgyxZsqRl6vEzExJ+/fXXpKamkpqaSlpamsOpTdpD7qNoRURcEk3aQn1+ptmlCNE3ffYwFO117j4HpsDcv7T59A033MBvfvMb7r77bgDef/991q5di7e3NytWrCAwMJCSkhLS09NZsGBBm2tHv/DCC/j6+nLgwAEyMjIYN25cy3NPPvkkoaGhWK1WZs+eTUZGBvfeey9PP/0069evJzw8/Af7amsa8ZCQkHZPZ37GzTffzLPPPsv06dN57LHHeOKJJ3jmmWf4y1/+Qk5ODl5eXi2XuxYvXsxzzz3H1KlTqa6uxtvbu93/zK2RFkUrBkcEc1RH4VEil56E6CvS0tI4deoUBQUF7Nmzh5CQEGJjY9Fa8/vf/54xY8Zw8cUXk5+fz8mTJ9vcz8aNG1v+YI8ZM4YxY8a0PPf+++8zbtw40tLS2LdvH/v3O55AtK1pxKH905mDMaFheXk506dPB+AXv/gFGzdubKnxpptu4q233sLd3fjuP3XqVH7729+yZMkSysvLW7Z3lrQoWuHlbiHXYxhTKjPMLkWIvsnBN//udN1117Fs2TKKioq44YYbAHj77bcpLi5mx44deHh4EBcX1+r04ueTk5PD4sWL2bZtGyEhIdxyyy2d2s8Z7Z3O/HxWrVrFxo0b+eSTT3jyySfZu3cvDz/8MPPmzWP16tVMnTqVtWvXMmrUqE7XKi2KNpQEjibIWgaVhWaXIoRopxtuuIH33nuPZcuWcd111wHGt/GIiAg8PDxYv349x44dc7iPiy66iHfeeQeAzMxMMjKML4yVlZX4+fkRFBTEyZMn+eyzz1re09YU521NI95RQUFBhISEtLRG3nzzTaZPn47NZiMvL4+ZM2fy1FNPUVFRQXV1NUePHiUlJYWHHnqIiRMntizV2lnSomiDHpQGp6HpxA48Rl9udjlCiHZISkqiqqqK6OhoBg0aBMBNN93E/PnzSUlJYcKECef9Zr1o0SJuvfVWEhMTSUxMZPz48QCMHTuWtLQ0Ro0aRWxsLFOnTm15z5133smcOXOIiopi/fr1Ldvbmkbc0WWmtrzxxhvcdddd1NbWMnToUF577TWsVisLFy6koqICrTX33nsvwcHB/OEPf2D9+vW4ubmRlJTE3LlzO/x5Z5NpxtuwasdR5nw8nrLx9zJgwZ+cVJkQrkumGe87OjrNuFx6asPI2AgO6xisJ3aaXYoQQphKgqINcWF+7GcYAWWZ4IKtLiGEaC8Jija4W9woDkjEr/k0VJwwuxwh+gRXvJTtajpzjiQoHGgeaIxx1gVy+UmI8/H29qa0tFTCohfTWlNaWtrhG/Bk1JMDQXFpNB2x0JCzHf/RV5hdjhC9WkxMDCdOnKC4uNjsUoQD3t7exMTEdOg9EhQOjIodwCEdS9TxHWaXIkSv5+HhQXx8vNlliG4gl54cGDUwgAxbPL6le6VDWwjRb0lQOBDg7UG+7yi8myuh3PHdnEII4aokKM6jIcI+IViBTDkuhOifJCjOI3DwWBq0O0150k8hhOifJCjOY1R0GAf0YOqPdW1KECGE6KskKM4jcVAgu2zD8Tm1C6xNZpcjhBA9ToLiPGJCfNjrnoS7tR4K95hdjhBC9DiXCgql1Hyl1IsVFRXO3CdVERONB8e+cdp+hRCir3CpoNBaf6K1vjMoKMip+40bEke2jsKaK0EhhOh/XCoousv4ISF8Zx2JPrYFbFazyxFCiB4lQdEO4waHsNWWiHtjFZxyvJi6EEK4GgmKdogI9CYv0JhJlmNbzC1GCCF6mARFO8XGjaCAAWjp0BZC9DMSFO00fkgI31pHYsv5RiYIFEL0KxIU7ZQ2OIRttlFY6kqg9IjZ5QghRI+RoGinUQMDyLAkGQ/k8pMQoh+RoGgnd4sbwTGJnFbB0qEthOhXJCg6YHxcqL2fYpP0Uwgh+g0Jig4YNySETdZk3KoKoPig2eUIIUSPkKDogHGxIWyw2u+nyPrc3GKEEKKHSFB0QJCvB34RQ8j3iIMjX5hdjhBC9AgJig6aGBfKusYU9PFvoaHa7HKEEKLbSVB00IKxUaxrGoOyNkLORrPLEUKIbidB0UGT40MpDBhLvfKGI9JPIYRwfRIUHeTmplgwIZ5NzUk0H/5chskKIVyeBEUnXDMumg22sbhX5kFJltnlCCFEt5Kg6IQhYX6cjpoOgM5aZ3I1QgjRvSQoOmnGpPFk2aKpylxjdilCCNGtJCg6aW7KQDaRim/hd1BfaXY5QgjRbSQoOinA24Oq+Mtw1000HlhtdjlCCNFtJCi6YNK0SynUoZR8v9TsUoQQottIUHRB+rBwtnheQHjRJmioMrscIYToFhIUXaCUwj35Kjxpomj7SrPLEUKIbiFB0UVTZ83jpA6mfNsHZpcihBDdQoKii8IDfDgQPIO48i001FaYXY4QQjidBIUThEy8Hm+a2LteWhVCCNcjQeEEKemXUkoI1r0rzC5FCCGcToLCCdzc3cmPuoQxdVs5kldgdjlCCOFUEhROEjfzl/ioRnavftnsUoQQwqkkKJwkMCGdIp8ERuYvp7CizuxyhBDCaSQonEUpvCbdSopbDqvXyUSBQgjXIUHhRCHpC2lUnvhnvk1lfZPZ5QghhFNIUDiTTzA1CfO5jM28v+WQ2dUIIYRTSFA4WciFdxCg6ij45h0amq1mlyOEEF3W64NCKTVUKfWKUmqZ2bW0y+B0agOHcXnTWlbuyje7GiGE6LJuDQql1KtKqVNKqcxzts9RSh1SSh1RSj3saB9a62yt9W3dWadTKYXPlNsY53aEr9evwWbTZlckhBBd0t0titeBOWdvUEpZgOeAucBo4KdKqdFKqRSl1Kfn/ER0c33dQo27mQbPEK6veot1+0+aXY4QQnRJtwaF1nojUHbO5knAEXtLoRF4D7hCa71Xa335OT+n2vtZSqk7lVLblVLbi4uLnXgUneAVgPu0+5hh2cP6Lz5Fa2lVCCH6LjP6KKKBvLMen7Bva5VSKkwp9Q8gTSn1SFuv01q/qLWeoLWeMGDAAOdV20mWyXdS7xnCvNLX+T7n3KwUQoi+o9d3ZmutS7XWd2mth2mt/8vsetrN0w/LtPu5yLKXL9d+ZHY1QgjRaWYERT4Qe9bjGPs2l+Mx+Q5qPcKYUfAymfmyVoUQom8yIyi2AcOVUvFKKU/gRuBjE+rofp6+uF30W6Za9rFxxYtmVyOEEJ3S3cNj3wW+BUYqpU4opW7TWjcD9wBrgQPA+1rrfd1Zh5m8L/gVRYEp/Lz4f9ibscvscoQQosOUK43IUUrNB+YnJCTckZWVZXY5LeqKc2h87kJK3QcS/9A3KA9vs0sSQogfUErt0FpPaO25Xt+Z3RFa60+01ncGBQWZXcoP+AyIZ0fqnxnafISCDx4wuxwhhOgQlwqK3mzq5TfznmU+0YffRG/+X3ChlpwQwrVJUPQQL3cL7pf+iU+t6agvHoM1j4DNZnZZQghxXhIUPeiqCfG8HPEfvKPmwfcvwLJboane7LKEEMIhCYoeZHFTPHnNWP7QsJBVg+6B/Svh9XlQWWh2aUII0SYJih6WFBXEL6fGcXfOBRyZ+Q84dQBemgn5O8wuTQghWuVSQaGUmq+UerGionffBf2bi0cQHezDv+2MounWtWDxgFfnwn7XvO9QCNG3tSsolFJ+Sik3++8jlFILlFIe3Vtax/XW4bHn8vNy509XJHH4ZDX3b2ii+bavYNAY+OAWyPzQ7PKEEOIH2tui2Ah4K6WigXXAzzHWmhCdNDsxkkfmjuLTjEJ+/dFxmn72IcROgg9vh4z3zS5PCCFatDcolNa6FrgaeF5rfR2Q1H1l9Q+/mj6MR+cl8llmEf+2LIuGG9+HIVNh+Z2wt2+s/CqEcH3tDgql1BTgJmCVfZule0rqX26fNpQnFiTx+f6TXPtKBrmXvmaExcpFcGyL2eUJIUS7g+I3wCPACq31PqXUUGB995XVv/zigjj+7+fjyTtdy2Uv7GDlyKfQwUPgvZ9ByRGzyxNC9HMdnhTQ3qntr7Wu7J6Sum7ChAl6+/btZpfRYYUVddy/dDffZZfxy9HwaNG9uHkFwO1fgl+Y2eUJIVxYlycFVEq9o5QKVEr5AZnAfqXUg84sUsCgIB/evj2dB34ygtcPwL36QWyVBfD6ZVCed/4dCCFEN2jvpafR9hbElcBnQDzGyKdepa/cR+GIxU1xz6zhvHtHOlubE7il8Xc0ns5Hv3wxFO4xuzwhRD/U3qDwsN83cSXwsda6Ceh105/2lfso2mPy0DBW3zcNFX8R82r+QFmdFf3qHDj0mdmlCSH6mfYGxf8BuYAfsFEpNQTotX0UriLc34vXb53IwvmXcmXDExxsGgjv3gjr/gDWJrPLE0L0E51e4U4p5W5f1rTX6aud2Y5knazigXe3cl3Jcyx0/5LmqIm4X/8aBMeaXZoQwgU4ozM7SCn1tFJqu/3nfzBaF6KHDI8M4IN7ZnLyov/i3uZ7aSjIpOn5qXDgU7NLE0K4uPZeenoVqAKut/9UAq91V1GidZ7ubvz7T0byy1/9O7/y+xsH6kNh6U1ULb9f1rUQQnSbdl16Ukrt1lqnnm9bb+GKl57OVd9k5bWNh/H++j+51W0Vpb5DCbz+BTzi0s0uTQjRB3X50hNQp5S68KwdTgXqnFGc6BxvDwuLZidy6W9f4bnov1BfU4Hl9TmULL0H6vvu8GAhRO/T3hbFWOCfwJlxp6eBX2itM7qxtk7rDy2Kc63fk03RR3/geusq6j2C8Zh+P56T7wBPX7NLE0L0AY5aFB0a9aSUCgTQWlcqpX6jtX7GSTU6hVJqPjA/ISHhjqysLLPL6XEVdU28+eEKxhxawkWWvTR4h+M17T5IWwi+oWaXJ4ToxZwWFOfs9LjWenCXKusm/bFFcbatOWW888FSrqt6k6mWfdgs3rilXAMTboPocaCU2SUKIXqZ7gqKPK11rxzE39+DAqCx2cZLm7L5Yv0XXKfXcY3HFrxsdTBorBEYKdeCp4xwFkIYpEXRj5VUN/Dc+iOs/O4gC9w2s8jvawbWHwWvIJhwC0z6FQRFm12mEMJknQ4KpVQVrc/ppAAfrbW7c0p0LgmKH8srq+X5DUf5cEceqRzi96EbGFu9EaXcIPlamP47CBtmdplCCJN0S4uiN5OgaFtRRT0vbcrm3a3HCW0q5OGQDcxpWIvF1ohKWwjTH5IWhhD9kASF+JGKuiY+2J7HG9/mUldWxKOBq1jQtBZlsaAu+DVceL/0YQjRj0hQiDZZbZrVewt55ovDNJTk8p8By5nZ+DUERMElfzI6vWWUlBAuT4JCnJfVpvlodz5/XXuImKo9LAlZyqCagzBsFsxfIrPUCuHinDGFh3BxFjfF1eNiWHv/RQwddzEXlD7K/3rfhfX49/B8Omx7BWw2s8sUQphAgkL8QKC3B09dO4bXbp3M29ZLuLjuvygMSIJVv4WlN0FdudklCiF6mEsFhSusmd1bzBgZwWf3TSN2aCJT8u9l2YC70Vnr4KWZUJRpdnlCiB7kUkHhSmtm9wZh/l68fstEHpqTyEP5F/KA75+xNtTAyxfDjtfBBfu3hBA/5lJBIZzPzU2xaMYwXr91Iuuqh3JZ/ZNUDUiDT+4z1u+uOml2iUKIbiZBIdpl2vABLF90AbVeYUzMu4eDqb+H7A1GR/feZdK6EMKFSVCIdhseGcDKf5vKyEHBXL41hQ0zPoCQIfDhbfDmVVB61OwShRDdQIJCdEiYvxdv3TaJtMHB/HJVFR+OewPm/hXyd8DzU+DL/4T6SrPLFEI4kQSF6LAAbw/e+OUkpgwL44EPM3lLXwr3bIPRC2DTYliSCt+9AM0NZpcqhHACCQrRKb6e7rzyi4nMHBnBoyszeXZrFfrql+DODRCZDGsehr9PlP4LIVyABIXoNG8PC//38/FcnRbN/3x+mCc+2Y9tYCrc/BEs/BC8Aoz+i5dmwdH1EhhC9FG9cj0J0Xd4WNxYfN1Ygn09efWbHMpqGvnrdWPwSrgYhs6EjKXw1Z/hzSth4Bi44F5IuhIsHmaXLoRoJ2lRiC5zc1P84fJEfjdnJB/vKeCWV7dRWd8EbhZI/Rn8eicseNbos1h+O/xvKmz5u3R6C9FHyOyxwqmW7zzB75ZlkBDhz2u3TmRQkM+/nrTZIGsdfPt3yN0EXoEw7maY8EtZXU8Ik8k046JHbc4q4a63duDt4caSG9O4ICH8xy/K3wlbnoX9H4G2GpepJtwKI+aCu2fPFy1EP9dvgkIpNR+Yn5CQcEdWVpbZ5fRrWSerWPT2TrKLq7n/4hHcPTMBN7dWFkCqLIRdbxpzR1Xmg284jL3RaGkMGNnjdQvRX/WboDhDWhS9Q01DM79fsZePdhdwYUI4T107huhgn9ZfbG2Go1/Czn/C4TVga4bYdCMwkq4CT9+eLV6IfkaCQphGa827W/P486r9uCnFf8xL5MaJsShHy6tWn4I97xqhUXrE6MtIuhJSb4LYybI0qxDdQIJCmC6vrJbfLcvg2+xSpiaE8ecrU4gP93P8Jq3h2Bbj0tT+j6GpBkKHwZR/g9SF4OHdM8UL0Q9IUIhewWbTvLP1OE99dpCGZhuLZgxj0YxheHtYzv/mhmo48LGxJGv+dvCLgPS7YOxPITCq+4sXwsVJUIhe5VRVPU+uOsBHuwuID/fjqWvGMCk+tH1v1hpyN8Pmvxl9GgAxEyFxAYycC2EJcmlKiE6QoBC90qasYn6/Yi95ZXXcPGUIv5szCn+vDkwWUJJlDK898DEU7jG2hcTDiEuNlkZUavcULoQLkqAQvVZtYzN/XXuI17fkMjDQm99eMoKrx8VgaW0orSPlx+HwWuOGvpyN0FxvdHxPuhMS54O7V/ccgBAuQoJC9Ho7jpXxxCf7yThRwYhIfx6aM4pZoyIcj45qS1057H4Htr0EZdngGQDDLzECY8Sl4HmeTnQh+iEJCtEnaK1ZvbeIxesOkVNSw4yRA/jj/CTizjc6qi02G2SvNy5PHVwFtSXg6W/cl5G2UIbaCnEWCQrRpzRZbbyxJZdnvsiisdnGnRcNZdGMYfh1pP/iXDarMdR2z3uwb4Ux1HZAIky5G1Kuk6G2ot+ToBB90qnKev7f6gOs3F3AgAAvHvjJCK4dH9vx/otzNVQbYfH9P+BkJvgNMAJj8iIJDNFvSVCIPm3HsdP8edV+dh0vZ9TAAB6aO4oZIwZ0rv/ibFobHd9blsCRLyBoMFzyR0i6Wi5JiX5HgkL0eVprVu0t5Kk1B8krqyN9aCgPzRlF2uAQ53xA9tew9j/g5F6IvwiueQX8I5yzbyH6AAkK4TIam228u/U4S77MorSmkYsTI7n/kuEkRQV1fec2K+x8A9Y8Aj6hcMNbEDO+6/sVog+QoBAup7qhmVc35/Dypmwq65uZmzyQm6fEMTk+tPXpzDuicA8sXQhVRTDrURhzAwQMdE7hQvRSEhTCZVXUNfHK5hxe25xDVUMz0cE+XJUWzc0XDCEioAsd07Vl8OHtP5wmZNQ8Y2GlASOlD0O4nH4TFLJwUf9V12hl3f4ilu/MZ1NWMd4eFu6aPow7pg3Fx7Mdkw62RmsoPggHP4UDn0LhbmN7SByMvAxGXQ6D0421wYXo4/pNUJwhLYr+LbekhqfWHOSzzCIGBnrz69kJXDs+Bi/3Lv5Br8g3FlU6vMbo/LY2GLPYjpxr3PkdPx28A51zEEL0MAkK0S9tzSnjvz47wK7j5UQGenHHtKH8bPJgfD27cOPeGQ1VxrxSBz6BrM+hsRqUBYZcAFPuMaYKkctTog+RoBD9ltaaLUdL+ftXR/g2u5Rwfy/umTmMn04e3PUWxhnWJsjbatyLkfkhlB+DQakw4xEJDNFnSFAIAWzPLWPxukN8l11GVJA398wazjXjo50XGGCERsZS2PhXOJ0LUeNg5n9AwmwJDNGrSVAIYae15psjpSxed4jdeeUMCvLmrunDuGFibPtW2msva5Mxr9TG/zamQI+ZZEx5PmoeePo673OEcBIJCiHOobVmU1YJz36Vxbbc0wR6u7MgNYprx8cyNiao69ODnNHcCLvfMlbkKz9uTHmedCWk/gwGT5FWhug1JCiEaIPWmq05Zby79TifZRbR0GxjTEwQj88fzfgh7VyetT1sNjj2Dex5F/atNGavDYmDsT+DMddB6FDnfZYQnSBBIUQ7VNY38cmeAp798ghFlfVclRbNQ3NGMTDIyTPKNlQbo6X2vGNMSghGX0bKtcbiSsGDnft5QrSDBIUQHVDT0MzzG47w0sYcrFpzwbAwFoyN4tLkgQR6ezj3w8rzYN9y2LsMijKMbeEjjfsyEmYbl6c8fJz7mUK0QoJCiE7IK6vlvW3H+WRPIcfLavFyd2Nu8kCunxBL+tCwrs8pda6SI5C11rgv49g3YG0Ed28jLIbNhKEzIDIF3Nyc+7lCIEEhRJdordmdV/d+jBMAABIPSURBVM7ynfms3J1PVX0zQ8J8ueWCOK6bEIt/V1bea0tjDeR+YyzlevQrYyoRMGa1HTbrX3eDe7cya67NBrYmcPdyfl3CZUlQCOEk9U1W1mQW8eZ3x9hx7DQB3u7cODGWm6fEERvajcNeKwuN/ozs9UaLo7YE3Dxg0FijTyMoBtBQsNuY/ba5HuIuhBFzjFCRfg9xHhIUQnSDXcdP88rmHD7LLMKmNbNHRXLLBXFMTQhz3vDa1tiscGIbHFxlhEJFHlScMJ6LTIaoNOOS1ZHPoeQwoIxJDKfcbUwxIkNyRSskKIToRoUVdbz13THe3ZpHWU0jydGBLJqewJzkgV1f37u9bDbQNrCccxms9KgxJHfbK1BXBgMSITwB/CMhKNYYZRU2rGdqFL2aBIUQPaC+ycrKXfn838ZsckpqiA/342eTBnPVuGjC/U3uL2ishYz3YP9HxoJMVUVQX248N3gKpN4EyVeDp5+5dQrTSFAI0YOsNs2azCJe3pzNruPluLspZo2K4Mq0aGaOjOj8+hjOVllohMeut6E0y+gYT/s5TLwNAu19Hihw9zS7UtEDJCiEMMmRU1V8sP0EH+7Mp6S6AV9PC5eMjmT+mCguGjEAT/deMNRVazj+LWx9EfZ/DNr6w+fDhhsd4/HTYPhPwCvAnDpFt5KgEMJkVpvm++xSPsko4LPMIsprmwj0dmdu8iCuHhfNpPjQ7u0Ab6/KAti3wrhUpRTYmiF/Bxz7FhqrwCvQaHVMvtOYgqSrmhuhqRZ8gru+L9ElEhRC9CKNzTa+OVLCx3sKWLeviJpGK0PCfLlufAzXjI9hUFAvvBPb2myMtNr2MuxfaXScD55i3AQ4dIYxTLc9921oDUV7jXtDcjYaLRlrI6QthIsetA/zFWaQoBCil6ptbGZNZhHvb8/ju+wy3BTMGBnBDRNjmTUqAg9LL7g0da6KfNjxunEXeWEGLX0ZQTEQGm+MpgoYCP4DjSnVmxuMMCg+CIfXQVWBsZ8BoyD+IiN0dv7T2MeEX8LU+yBwkHnH109JUAjRBxwvrWXp9uN8sP0Ep6oaCPB2Z/qIAcxOjGD6iAhC/Xphp3JtmdEyOHUAyrKNn8p8qD71474OzwBjKpIRl0LCxUaYnFF+HL7+b9j9DrhZjMtbU++DkCE9ezz9mASFEH1Is9XGhkPFrNtfxFcHiympbkApGBMdxPQRA5g+cgCpsSE9d49GZ9isUFtq9D+4e4PF0+jfOPc+j3OV5cA3zxgjsWxNxvTrg1KNS1uhQ43gCIiChkojjOrKIHr8D0NHdIoEhRB9lM2myciv4OtDxWzMKmbX8dPYNIT4ethbG5HMHBXRPfNNmaki3xi6W7DLmJakIs/BixUMmWosCDXqcrls1Un9JiiUUvOB+QkJCXdkZWWZXY4QTldR28TGrGLWHzzFhsPFlNU04unuxkXDw5mbPIhLkiKdPxV6b1BbZlyeKj9ujMzyDgL/CPD0h+wNxlTtZyZOjJloLDkbO9mYst0vzNTS+4p+ExRnSItC9AdWm2bHsdN8llnI2swiCirq8bS4cdGIcH6SNJBxg4MZGu7v/OnQe6tTB+HgJ8YcWAW7/rXdJxQCBhn3f5y5B6Sp1piht7ne6GxvbjBuLAyMhsAoI2BGzoXIpH4zN5YEhRAu7sxU6KsyClm1t5DCinoAArzcSR0czIKxUcxNGeR6l6jaUlkIJzONSRGLDxn9JQ2VUF9p/OH38DMWhPLwNvpQ3L2gqd5orVTmGy0XtDHr7rDZ/xrN5RMM1cVQVWjs083deK+7txEwQbFGP0pglNn/Ah0mQSFEP2KzaY4UV7Mnr5w9J8rZlFXCsdJavD3cSB8aRnV9M0WV9ZTXNhHq50lkoBdRwT5ckRrFjBER/acF4kj1KTj0mdE6yfsO6it+/Bp3b+OGRFvzj5+LTDHWQk++FoKiu79eJ5CgEKIf01qz8/hplu/MZ2tOGWH+ngwM9CbY15PTtY2cqmwg61Q1JdUNDBvgx20XDuXqcdF4e/SSOal6g/pKo0O9rtyYeTdgIHj5G8/ZbNBUY7RGKvKMS2D7VkC+/W9Q9HgYMdcYFhyZ3PYKhUV74ft/gLIYnfLxFxktnh4iQSGEcKjJamP13kJe2pRNZn4lIb4e3DR5CD+fMoTIwJ77Y+VSSo8aneyH1hjToKDBOxhiJkDMJAiINFolKNj7Phz5wuicRxnTpXj6w+grjHVEIpO6vVwJCiFEu2it2ZpTxiubc/j8wEksSjFuSAjp8aFMHhrG+CEh0tLojOpTxsqEed9B3jb7CK2z/vb6DYD0RTDhNqPvJGcjHPgY9i4zOt6HzTKmgh+UatxP0g3rpktQCCE67FhpDe9uzWPL0RIy8yuwafByN/o5po8YQPrQMEZE+uPeG6cZ6e0aqo1+D6t9xFVIfOuXmWrLYMdr8P2LUF1kbPPwg4hRxqy+4QlGK+V0rnGzYmU+3LmhUyO1JCiEEF1SVd/E9tzTbMoq4evDpzhaXAOAr6eFsTHBpA4OZkx0ECkxQUQH+/SOmXBdibUJTu4zRnIV7TWmTCk9YgQDgLuPMZtvaDxc/dK/+k86QIJCCOFUJ07XsuPYaXYeO83O4+UcLKqkyWr8LQnz8yQlJogx0UGMjQ1m/JAQgn174TxVrqChGhqrjQ72Loazo6DoJ4OqhRDOFBPiS0yIL1ekGkM/65usHCqqIuNEORknKtibX8HGw8XY7N9DR0T6c8GwcOaPjWLc4GBpcTiLl3+nWg8dJS0KIUS3qG1sZk9eBTuOlbE19zTfZZfS2GwjNtSHGSMiCPP3JMjHgzB/L2JDfBgc6kuon6eEiEmkRSGE6HG+nu5MGRbGlGHGXEtV9U2s23eSj/YUsHJ3PlX1P75RLdjXg4sTI5mXMoipCeG9Y6lYIS0KIYQ5mq02quqbKa5uIK+slmOltezNr+CL/SepamjGz9NCQoQ/wwb4MyzCn6SoQFKigwjzb8dKeqLDpEUhhOh13C1uhPh5EuLnyYjIgJbtDc1WvjlSwteHijlaXMO32aUs35Xf8nx0sA9JUYEkRQWRHB0oneU9QIJCCNGreLlbmDUqklmjIlu2VdY3sS+/ksz8CjLyK9hXUMHnB06itTHYJykqkKnDwpkQF8q4wcHS6nAyufQkhOiTahqaycyv4LvsMrYcLWHX8XIarTYAhoT5MnpQIMMjAxge4U9KdBBDwnylo9wBuY9CCOHy6pus7M2vYOex0+w6Xs6hk1UcK61pGaIb7u/FhCEhTIgLIW1wCMnRgXi5y3QkZ0gfhRDC5Xl7WJgYF8rEuNCWbfVNVo4WV7Mnr4LtuWVsO1bGmn3GVBieFjeGRfgTE+JDdLAPg4K8CfXzJMzfk+hgX4ZH9KNFn85DgkII4bK8PSwkRQWRFBXEzyYPBuBUVT07j5Wz6/hpjpyq5nhpLVuOlFDTaP3BewO93ZkQF0pqbDCDQ32JDvEhNsSXyECvfncJS4JCCNGvRAR4Myd5IHOSB7Zs01pT02ilrLqR0poGsotr2H6sjK05ZXx18NQP3u/naSF+gB/x4f7EhvgQE+JLVLDRGgnx9STUzxM/F1tJUPoohBDCgbpGK/nldZw4Xcvxslqyi2vILqkht6SGgvI6mm0//hsaE+JDSnQQydFBJEUFMnpQIAMCendLRPoohBCik3zsN/4lRPx4TiWrTXOqqp6C8jrKapo4XdtIcVUD+wsq2ZtfwWeZRS2vDfXzxNfTwpnv5uEBXgwO9WVwqNEqGRTkTXSwD0G+Hvh4WPD2sODRS6Zwl6AQQohOsrgpBgX5MCjIp9XnK2qbOFBUyYHCSg6frKKhyQYK0HCqqoE9eeWs3luItZVWCUBkoBcT40KZHB9K2uAQEiL8TVk4SoJCCCG6SZCvB+lDw0gfGtbma5qtNk5VNVBQXkd+eR2VdU3UN9moa7Jy5FQ1W3PK+DSjEDCCadgAP4ZHBjA41Jchob4MDvNlaLh/t3ayS1AIIYSJ3C1uRAX7EBXsQ2sdBFpr8srqyMgv52BhFQeLKtmXX8HazKIf9I/4elqIC/Nj6a/SCfD2cG6NTt2bEEIIp1JKMTjMaDlcPuZf2602TWFFHbklteSU1pBTXEN+eS3+3TDiSoJCCCH6IIuballA6sLh4d36Wb2jS10IIUSvJUEhhBDCIQkKIYQQDklQCCGEcEiCQgghhEMSFEIIIRySoBBCCOGQBIUQQgiHXHKacaVUMXCsk28PB0qcWE5f0B+PGfrncffHY4b+edwdPeYhWusBrT3hkkHRFUqp7W3Nye6q+uMxQ/887v54zNA/j9uZxyyXnoQQQjgkQSGEEMIhCYofe9HsAkzQH48Z+udx98djhv553E47ZumjEEII4ZC0KIQQQjgkQSGEEMIhCQo7pdQcpdQhpdQRpdTDZtfTXZRSsUqp9Uqp/UqpfUqp++zbQ5VSnyulsuz/DTG7VmdTSlmUUruUUp/aH8crpb63n/OlSilPs2t0NqVUsFJqmVLqoFLqgFJqiqufa6XU/fb/tzOVUu8qpbxd8VwrpV5VSp1SSmWeta3Vc6sMS+zHn6GUGteRz5KgwPgDAjwHzAVGAz9VSo02t6pu0wz8u9Z6NJAO3G0/1oeBL7XWw4Ev7Y9dzX3AgbMePwX8TWudAJwGbjOlqu71v8AarfUoYCzG8bvsuVZKRQP3AhO01smABbgR1zzXrwNzztnW1rmdCwy3/9wJvNCRD5KgMEwCjmits7XWjcB7wBUm19QttNaFWuud9t+rMP5wRGMc7xv2l70BXGlOhd1DKRUDzANetj9WwCxgmf0lrnjMQcBFwCsAWutGrXU5Ln6uMZZ49lFKuQO+QCEueK611huBsnM2t3VurwD+qQ3fAcFKqUHt/SwJCkM0kHfW4xP2bS5NKRUHpAHfA5Fa60L7U0VApElldZdngN8BNvvjMKBca91sf+yK5zweKAZes19ye1kp5YcLn2utdT6wGDiOERAVwA5c/1yf0da57dLfOAmKfkop5Q98CPxGa1159nPaGDPtMuOmlVKXA6e01jvMrqWHuQPjgBe01mlADedcZnLBcx2C8e05HogC/Pjx5Zl+wZnnVoLCkA/EnvU4xr7NJSmlPDBC4m2t9XL75pNnmqL2/54yq75uMBVYoJTKxbisOAvj2n2w/fIEuOY5PwGc0Fp/b3+8DCM4XPlcXwzkaK2LtdZNwHKM8+/q5/qMts5tl/7GSVAYtgHD7SMjPDE6vz42uaZuYb82/wpwQGv99FlPfQz8wv77L4CPerq27qK1fkRrHaO1jsM4t19prW8C1gPX2l/mUscMoLUuAvKUUiPtm2YD+3Hhc41xySldKeVr/3/9zDG79Lk+S1vn9mPgZvvop3Sg4qxLVOcld2bbKaUuw7iObQFe1Vo/aXJJ3UIpdSGwCdjLv67X/x6jn+J9YDDGFO3Xa63P7Sjr85RSM4AHtNaXK6WGYrQwQoFdwEKtdYOZ9TmbUioVowPfE8gGbsX4guiy51op9QRwA8YIv13A7RjX413qXCul3gVmYEwnfhJ4HFhJK+fWHpp/x7gMVwvcqrXe3u7PkqAQQgjhiFx6EkII4ZAEhRBCCIckKIQQQjgkQSGEEMIhCQohhBAOSVAI0QlKKatSavdZP06bWE8pFXf2jKBCmM39/C8RQrSiTmudanYRQvQEaVEI4URKqVyl1H8rpfYqpbYqpRLs2+OUUl/Z1wL4Uik12L49Uim1Qim1x/5zgX1XFqXUS/Z1FdYppXxMOyjR70lQCNE5PudcerrhrOcqtNYpGHfCPmPf9izwhtZ6DPA2sMS+fQnwtdZ6LMY8TPvs24cDz2mtk4By4JpuPh4h2iR3ZgvRCUqpaq21fyvbc4FZWuts++SLRVrrMKVUCTBIa91k316otQ5XShUDMWdPJ2Gf/v1z++IzKKUeAjy01n/u/iMT4sekRSGE8+k2fu+Is+chsiL9icJEEhRCON8NZ/33W/vvWzBmrgW4CWNiRjCWq1wELWt6B/VUkUK0l3xLEaJzfJRSu896vEZrfWaIbIhSKgOjVfBT+7ZfY6w09yDGqnO32rffB7yolLoNo+WwCGNlNiF6DemjEMKJ7H0UE7TWJWbXIoSzyKUnIYQQDkmLQgghhEPSohBCCOGQBIUQQgiHJCiEEEI4JEEhhBDCIQkKIYQQDv1/ihioJa2/aDoAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"Rot49hATDk4K"},"source":["## Tuning\n","The model is learning, but we can do better. \n","\n","**At this point, you can play with the above code to change hyperparameters and note changes to your results.**\n","\n","Alternatively, you can step through tuning this model below.\n","\n","## Walk-through tuning:\n","Perhaps we did not have enough model parameters to accurately represent the mapping. Remedy this by increasing the number of hidden neurons to 20."]},{"cell_type":"code","metadata":{"id":"N3SfAb-mDk4L"},"source":["#Use the same code as in the previous cell\n","#simply change the number of neurons in the hidden layer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PQWoQKonDk4L"},"source":["We see that we got little improvement here. Another hyperparameter to adjust is *batch size*, which is the number of training examples used to calculate the gradient on each step. While you may initially think that a higher batch size leads to faster or more accurate training, in practice this is not true. The \"noise\" that arises from using less training examples at each iteration can actually help find the global minimum of the loss function.\n","(See here for more info: https://arxiv.org/pdf/1609.04836.pdf)\n","\n","Try decreasing the batch size to 16."]},{"cell_type":"code","metadata":{"id":"UB9__PwjDk4M"},"source":["#Complete me:"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q4QGMh3-Dk4M"},"source":["This is starting to do better but has significant room for improvement.\n","\n","Another hyperparameter to tune is the *learning rate*. \n","\n"," - If the learning rate is too high, we are taking too large of a step in the gradient descent at each iteration and will miss narrow minima in the loss function. \n"," - If the learning rate is too small, then we are not traveling far enough in each iteration and we will take far too long to reach a minimum. \n","\n","Perhaps the learning rate is too high and the network can't fine tune. Try decreasing the learning rate to 0.001."]},{"cell_type":"code","metadata":{"id":"s851AM6GDk4N","colab":{"base_uri":"https://localhost:8080/","height":249},"executionInfo":{"status":"error","timestamp":1610334233276,"user_tz":300,"elapsed":778,"user":{"displayName":"Michelle Kuchera","photoUrl":"","userId":"09037640902854628765"}},"outputId":"187205f1-3812-49aa-b28d-946fc5f9d637"},"source":["\n","#Complete me:\n","model = tf.keras.Sequential() #Define the model object\n","model.add(tf.keras.layers.Dense(20, input_shape=(5,), activation=\"relu\")) #Add the hidden layer\n","\n","#Add the output layer yourself\n","#It should not have an activation function\n","#Complete me:\n","model.add(tf.keras.layers.Dense(1))\n","\n","model.compile(tf.keras.optimizers.Adam(lr=0.001),loss=tf.keras.losses.MeanSquaredError()) #Adam optimizer and mean squared error loss\n","results = model.fit(smallEnergy, smallTarget, epochs=30, batch_size=16, validation_split=0.8)\n","\n","plot_learning_curve(results.history)\n","\n"],"execution_count":69,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-69-28219b579216>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Adam optimizer and mean squared error loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmallEnergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmallTarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'smallEnergy' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"yh748SgrDk4O"},"source":["This is not really that much better, but now there is evidence of *overtraining* or *overfitting* -- the training loss is so much lower than the validation loss. \n","\n","A common fix to this is adding *dropout layers*. Try adding a dropout layer with dropout rate of 0.5. <https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout>\n","\n","You can also try batch normalization: <https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization>"]},{"cell_type":"code","metadata":{"id":"XO_Ou80xDk4O"},"source":["#Dropout layers are located under tf.keras.layers. \n","#They take the dropout rate as their only argument.\n","#BatchNormalization layers are also under tf.keras.layers, and in the simplest use case, take no arguments\n","\n","#Complete me:\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Owd0k7kEDk4P"},"source":["This clearly stopped the overtraining problem, but it still isn't training well. Now, try training on the full dataset with a more reasonable validation split of 0.2. Use a single hidden layer with 20 neurons, a learning rate of 0.001, and a batch size of 256. Just run it for 10 epochs."]},{"cell_type":"code","metadata":{"id":"iStR0aT5Dk4P"},"source":["#Complete me:\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3vkQ1kX2Dk4Q"},"source":["This clearly resulted in a significant improvement and shows how important having a large enough dataset is. Moving on to the choice in activation functions, ReLU is not the only available choice, although it is one of the most popular ones currently. Try training a network using a sigmoid or tanh activation function."]},{"cell_type":"code","metadata":{"id":"B3qtiBmiDk4Q"},"source":["#Simply change relu to sigmoid or tanh to change the activation function\n","\n","#Complete me:\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZRlAj8KFDk4R"},"source":["Next, try adding 2 new hidden layers to the network. Use the ReLU activation function."]},{"cell_type":"code","metadata":{"id":"oJlPFnsjDk4R"},"source":["#Complete me:\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t3lUeX2dDk4S"},"source":["Clearly, adding more layers helps improve the quality of the network. There is a limit to how effective this is though. Try having 5 hidden layers."]},{"cell_type":"code","metadata":{"id":"3kKOzBCrDk4S"},"source":["#Complete me:\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"psJeOPOsDk4S"},"source":["Now, see what happens when you increase the number of neurons per layer from 20 to 50 in the 3 hidden layer model. Consider how they perform compared to ReLU now. "]},{"cell_type":"code","metadata":{"id":"k430N-vIDk4T"},"source":["#Complete me:\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ntl38Zo7Dk4U"},"source":["Try using the sigmoid and the tanh activation functions again and compare them to ReLU."]},{"cell_type":"code","metadata":{"id":"JHn1MKIiDk4V"},"source":["#Complete me:\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D-HNQQhpDk4V"},"source":["This difference in performance, especially with the sigmoid function, is known as the vanishing gradient problem. If the value for any one the neurons gets too far away from 0, the gradient for sigmoid and tanh gets really close to 0. This means that for deeper networks it is much more difficult to update the weights in the earlier layers as their gradient is so small. Now, remove the fifth column from the input data, the charge, and see what happens when training. Why do you think including charge has such a large impact?"]},{"cell_type":"code","metadata":{"id":"ceRTx4CpDk4W"},"source":["#Complete me:\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IscwBDy7Dk4W"},"source":["Finally, there are other options for the loss function. Try experimenting with alternatives to mean squared error.\n","\n","<https://www.tensorflow.org/api_docs/python/tf/keras/losses>\n","\n","You can also try some other optimizers -- for example, sgd (with and without momentum), rmsprop, adagrad, adadelta, adamax, and nadam. <https://www.tensorflow.org/api_docs/python/tf/keras/optimizers>\n","\n"]},{"cell_type":"code","metadata":{"id":"uCm5VSXDDk4X"},"source":[""],"execution_count":null,"outputs":[]}]}